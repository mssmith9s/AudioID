# Install necessary packages 
!pip install librosa tensorflow pydub

import os
import numpy as np
import librosa
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix

def extract_features(file_path, sample_rate=22050, n_mfcc=13, max_len=1000):
    """
    Extracts MFCC features from an audio file and computes delta and delta-delta features.
    
    Parameters:
    - file_path (str): Path to the audio file.
    - sample_rate (int): Sampling rate for audio processing.
    - n_mfcc (int): Number of MFCC features to extract.
    - max_len (int): Maximum length for padding/truncating features.
    
    Returns:
    - np.ndarray: Array of extracted features.
    """
    try:
        print(f"Processing file: {file_path}")

        # Check if the file exists
        if not os.path.exists(file_path):
            print(f"File not found: {file_path}")
            return None
        
        # Load the audio file using librosa directly
        y, sr = librosa.load(file_path, sr=sample_rate)
        
        # Extract MFCC features
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
        delta_mfccs = librosa.feature.delta(mfccs)
        delta2_mfccs = librosa.feature.delta(mfccs, order=2)

        # Concatenate the MFCC features with delta and delta-delta features
        features = np.concatenate((mfccs, delta_mfccs, delta2_mfccs), axis=0)

        # Pad or truncate features to ensure uniform length
        if features.shape[1] > max_len:
            features = features[:, :max_len]
        elif features.shape[1] < max_len:
            padding = max_len - features.shape[1]
            features = np.pad(features, ((0, 0), (0, padding)), mode='constant')

        # Flatten the features to be 1D
        return features.flatten()

    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        return None

def load_data(dataset_path, max_len=1000):
    """
    Loads audio data from the dataset directory and extracts features.
    
    Parameters:
    - dataset_path (str): Path to the dataset directory.
    - max_len (int): Maximum length for padding/truncating features.
    
    Returns:
    - np.ndarray: Array of extracted features.
    - np.ndarray: Array of labels corresponding to the features.
    """
    X = []
    y = []

    for dirpath, _, filenames in os.walk(dataset_path):
        for file_name in filenames:
            if file_name.endswith(".mp3"):
                file_path = os.path.join(dirpath, file_name)
                
                # Debugging: Check if file exists before processing
                if not os.path.exists(file_path):
                    print(f"File does not exist: {file_path}")
                    continue
                
                features = extract_features(file_path, max_len=max_len)
                if features is not None:
                    X.append(features)
                    label = os.path.basename(dirpath)
                    y.append(label)
    
    return np.array(X), np.array(y)

# Test the function
dataset_path = r'C:\Users\Mixed\Desktop\CS505\FinalProject\dataset'
X, y = load_data(dataset_path)

# Encode labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Train an SVM model
clf = SVC(kernel='linear')
clf.fit(X_train, y_train)

# Make predictions
y_pred = clf.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Confusion Matrix:\n{conf_matrix}")

print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
